https://docs.google.com/document/d/1cR_7iWAAFllu3a0MFRKzUQhFFyVvPbH2BEyLdRgYRFQ/edit?tab=t.0#heading=h.p0qoxf2ghyz1

# MOMOKIES

MOMOKIES là projects docker-compose chứa các docker cơ bản để test việc đồng bộ data từ mongo -> elasticsearch của module profiling:

* Setup a standalone elasticsearch:7.17.6
* Setup a standalone kibana:7.17.6
* Setup a cluster single node mongo:4.4
* Setup monstache:6.7.10

## Mục Đích
* Test việc đồng bộ giữa Mongo vs ElasticSearch
* Test việc đồng đồng bộ child documents và đảm bảo khi các child documents được thêm mới, sẽ không sinh ra các deleted docs trên ElasticSearch

# chạy dockerfile 
docker build -t monstache_profile .

# coppy file so 
docker cp 390f8770a195:/bin/profiling/plugin_profile.so /home/toandd/Documents/profilingmonstache/plugins/plugin_profile.so

## Using Docker Compose

```bash
$ docker-compose up -d
```

Nếu MongoDB không khởi tạo được cluster bằng script rs-init.sh, khởi tạo thủ công bằng cách sau:
```bash
$ docker-compose exec mongo bash
$ sh scripts/rs-init.sh
```

Nếu MongoDB không khởi tạo được user, sử dụng các command line sau đây để khởi tạo:
```bash
$ docker-compose exec mongo mongosh
$ use admin
$ db.createUser({user: "admin", pwd: "admin",roles:[{role: "userAdminAnyDatabase" , db:"admin"}]}) 
```

Sau khi khởi tạo các docker, sử dụng CURL trong folder resources để tạo mapping cho index profiling. 

# tạo primary 
cfg = rs.conf()
cfg.members[0].host = "localhost:27017"
rs.reconfig(cfg)

cfg = rs.conf()
cfg.members[0].host = "mongo:27017"
rs.reconfig(cfg)




docker exec -it spark /opt/spark/bin/spark-shell --master spark://spark:7077 --jars /opt/spark/jars/mongo-spark-connector_2.12-3.0.2.jar
docker cp ~/Documents/pipeline_mongo/spark-jars/mongodb-driver-sync-5.4.0.jar spark:/opt/spark/jars/
docker cp ~/Documents/pipeline_mongo/spark-jars/scala-library-2.13.16.jar spark:/opt/spark/jars/
docker cp ~/Documents/pipeline_mongo/spark-jars/bson-5.4.0.jar spark:/opt/spark/jars/
docker cp ~/Documents/pipeline_mongo/spark-jars/mongodb-driver-core-5.4.0.jar spark:/opt/spark/jars/
docker compose restart spark
docker exec -it spark /opt/spark/bin/spark-shell --master spark://spark:7077

val oplogDF = spark.read
  .format("mongo")
  .option("uri", "mongodb://admin:admin@mongo:27017/admin?authSource=admin")
  .option("collection", "oplog.rs")
  .load()
oplogDF.show()